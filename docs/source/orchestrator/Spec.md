# Unified orchestrator

... is a system that schedules a list of chronon conf-s or queries with the following approach.

- planned
  - each conf (query) that a user writes is planned into a set of nodes. see: `Join*Planner.scala` and `GroupBy*Planner.scala`
  - nodes could be common between queries. we will guarantee that there is no race conditions.
  
- can operate in multiple modes
  - the main modes we support are offline & online
  - the planner creates offline nodes and online nodes from a conf (with potential overlap)
  
- lineage based 
  - nodes are connected to each other - through 
    - output tables they produce - `node.metaData.outputTable` and
    - their table dependencies `node.metaData.executionInfo.tableDependencies`.
  - we maintain a global lineage graph of nodes that we must execute in topological order
  
- partition aware
  - table dependencies on nodes will provide the following details needed to compute a input partition range for a given output partition range
    - startOffset, endOffset, startCutoff, endCutoff
    - formula: 
      - input_start = `max(output.start - startOffset, startCutoff)`,
      - input_end = `min(output.end - endOffset, endCutoff))`
    - NOTE:  
      - startOffset could be null in which case we just want to select from startCutOff if specified, or all partitions
      - we will treat the output.end as endCutOff if unspecified
    - This logic is all implemented in `DependencyResolver.scala`

- "step" aware
  - we want to typically be computing a large range of partitions - say 180day in steps of 7 days (or less in the final step)
  - each node of the lineage graph has its own step size
    - so it is possible that one step in a downstream node could depend on multiple steps of the SAME upstream node
    - it is also possible that an upstream node's step could be a dependency for multiple steps of the same downstream node
    - example: 
      - setup:
        - let's say a chronon join depends on a chronon groupBy that does a 90 day aggregation on an event source generated by a staging query
        - we want to compute 30 partitions of this join
        - for efficiency the groupBy computes steps of 7 days/partitions at a time
        - while the join and staging query compute 1 partition at a time
      - requirement:
        - we need to schedule 90 + 30 tasks of staging query with step size 1 - to create 120 partitions for groupBy
        - we need to schedule 5 tasks of groupBy with step size 7 (size 2 in the end - `4 * 7 + 1 * 2 = 30`)
        - we need to schedule 30 tasks of the join with step size 1
        - the groupBy tasks will be waiting for multiple staging queries at once - each GB waits for 7 SQs
        - 7 join tasks will be waiting on the same groupBy query - and will get unblocked once that finishes
  
- race-free
  - we could have multiple queries being run at the same time - that depend on the same node upstream
  - for the overlapping node - we should ensure that duplicate runs aren't triggered
  - we should also ensure that if two incoming steps partially overlap we DO schedule union of the range, while not duplicating in the intersection of the range.

- cancellable
  - if two active runs depends on the same upstream step - cancelling one of the steps should not kill the upstream job
  - but if only one active run depends on the step, it is okay to cancel it, and kill the underlying job


## Design Principles

The scale here is going to be very low - given that it is operating on a small number of active jobs.
So the orchestrator should be run as a single process on a single server accepting scheduling requests backed by a relational database.

All the code described below should be in scala as case classes for tables (slick), requests and responses.

At a high level we need to create a sync flow that pushes user local configs/queries to the remote. We work with git to ensure tracking:
   1. Remote will handle list of incoming `UploadRequest(commit, branch, user, Map[conf_name, hash])`, and check this against the existing `contents` table with schema `(conf_name, hash, content, commit, branch, user, timestamp)`
         1. Local cli should ensure that a sync is blocked when the local git status is dirty
   2. Remote returns `statuses: List[conf_status:(conf, hash, diff)]` to local for only the newly added or updated confs by user
      1. diff is None for newly added confs, but computed against "latest_main(conf_name)": existing-most-recently-updated-conf-on-main.
   3. for non-main incoming branches - 
      1. once the user confirms -  the local then sends up `List(conf, hash, content, commit, branch, user)` to be added as new rows into the table.
   4. for main incoming branch
      1. we will update 

2. the concept of a "run" - of a conf with all its dependencies for a particular date.
2. the run invokes the planner and creates nodes of un-planned confs.
3. we then recursively find the dependencies of the terminal node of the conf.
4. we split the unscheduled & missing

