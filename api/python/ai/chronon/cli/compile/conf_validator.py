"""Object for checking whether a Chronon API thrift object is consistent with other
"""

#     Copyright (C) 2023 The Chronon Authors.
#
#     Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.

import json
import logging
import re
import sys
import textwrap
from collections import defaultdict
from dataclasses import dataclass
from typing import Dict, List, Tuple

import ai.chronon.api.common.ttypes as common
from ai.chronon.api.ttypes import (
    Accuracy,
    Aggregation,
    Derivation,
    EventSource,
    GroupBy,
    Join,
    JoinPart,
    Source,
)
from ai.chronon.cli.compile.column_hashing import (
    compute_group_by_columns_hashes,
    get_pre_derived_group_by_columns,
    get_pre_derived_group_by_features,
    get_pre_derived_join_features,
    get_pre_derived_source_keys,
)
from ai.chronon.cli.compile.version_utils import is_version_change
from ai.chronon.logger import get_logger
from ai.chronon.repo.serializer import thrift_simple_json
from ai.chronon.utils import get_query, get_root_source

# Fields that indicate status of the entities.
SKIPPED_FIELDS = frozenset(["metaData"])
EXTERNAL_KEY = "onlineExternalParts"


@dataclass
class ConfigChange:
    """Represents a change to a compiled config object."""
    name: str
    obj_type: str
    online: bool = False
    production: bool = False
    base_name: str = None
    old_version: int = None
    new_version: int = None
    is_version_change: bool = False


def _filter_skipped_fields_from_join(json_obj: Dict, skipped_fields):
    for join_part in json_obj["joinParts"]:
        group_by = join_part["groupBy"]
        for field in skipped_fields:
            group_by.pop(field, None)
    if EXTERNAL_KEY in json_obj:
        json_obj.pop(EXTERNAL_KEY, None)


def _is_batch_upload_needed(group_by: GroupBy) -> bool:
    if group_by.metaData.online or group_by.backfillStartDate:
        return True
    else:
        return False


def is_identifier(s: str) -> bool:
    identifier_regex = re.compile(r"[a-zA-Z_][a-zA-Z0-9_]*")
    return re.fullmatch(identifier_regex, s) is not None


def _source_has_topic(source: Source) -> bool:
    if source.events:
        return source.events.topic is not None
    elif source.entities:
        return source.entities.mutationTopic is not None
    elif source.joinSource:
        return _source_has_topic(source.joinSource.join.left)
    return False


def _group_by_has_topic(groupBy: GroupBy) -> bool:
    return any(_source_has_topic(source) for source in groupBy.sources)


def _group_by_has_hourly_windows(groupBy: GroupBy) -> bool:
    aggs: List[Aggregation] = groupBy.aggregations

    if not aggs:
        return False

    for agg in aggs:

        if not agg.windows:
            return False

        for window in agg.windows:
            if window.timeUnit == common.TimeUnit.HOURS:
                return True

    return False


def detect_feature_name_collisions(
        group_bys: List[Tuple[GroupBy, str]],
        entity_set_type: str,
        name: str
) -> BaseException | None:
    # Build a map of output_column -> set of group_by name
    output_col_to_gbs = {}
    for gb, prefix in group_bys:
        jp_prefix_str = f"{prefix}_" if prefix else ""
        key_str = "_".join(gb.keyColumns)
        prefix_str = jp_prefix_str + key_str + "_"
        cols = compute_group_by_columns_hashes(gb, exclude_keys=True)
        if not cols:
            print('HERE')
        cols = {
            f"{prefix_str}{base_col}"
            for base_col in list(compute_group_by_columns_hashes(gb, exclude_keys=True).keys())
        }
        gb_name = gb.metaData.name
        for col in cols:
            if col not in output_col_to_gbs:
                output_col_to_gbs[col] = set()
            output_col_to_gbs[col].add(gb_name)

    # Find output columns that are produced by more than one group_by
    collisions = {
        col: gb_names
        for col, gb_names in output_col_to_gbs.items()
        if len(gb_names) > 1
    }

    if not collisions:
        return None  # no collisions

    # Assemble an error message listing the conflicting GroupBys by name
    lines = [
        f"{entity_set_type} for Join: {name} has the following output name collisions:\n"
    ]
    for col, gb_names in collisions.items():
        names_str = ", ".join(sorted(gb_names))
        lines.append(f"  - [{col}] has collisions from: [{names_str}]")

    lines.append(
        "\nConsider assigning distinct `prefix` values to the conflicting parts to avoid collisions."
    )
    return ValueError("\n".join(lines))


class ConfValidator(object):
    """
    applies repo wide validation rules
    """

    def __init__(
        self,
        input_root,
        output_root,
        existing_gbs,
        existing_joins,
        existing_staging_queries,
        log_level=logging.INFO,
    ):

        self.chronon_root_path = input_root
        self.output_root = output_root

        self.log_level = log_level
        self.logger = get_logger(log_level)

        # we keep the objs in the list not in  a set since thrift does not
        # implement __hash__ for ttypes object.

        self.old_objs = defaultdict(dict)
        self.old_group_bys = existing_gbs
        self.old_joins = existing_joins
        self.old_staging_queries = existing_staging_queries
        self.old_objs["GroupBy"] = self.old_group_bys
        self.old_objs["Join"] = self.old_joins
        self.old_objs["StagingQuery"] = self.old_staging_queries

    def _get_old_obj(self, obj_class: type, obj_name: str) -> object:
        """
        returns:
           materialized version of the obj given the object's name.
        """
        class_name = obj_class.__name__

        if class_name not in self.old_objs:
            return None
        obj_map = self.old_objs[class_name]

        if obj_name not in obj_map:
            return None
        return obj_map[obj_name]

    def _get_old_joins_with_group_by(self, group_by: GroupBy) -> List[Join]:
        """
        returns:
            materialized joins including the group_by as dicts.
        """
        joins = []
        for join in self.old_joins.values():
            if join.joinParts is not None and group_by.metaData.name in [
                rp.groupBy.metaData.name for rp in join.joinParts
            ]:
                joins.append(join)
        return joins

    def can_skip_materialize(self, obj: object) -> List[str]:
        """
        Check if the object can be skipped to be materialized and return reasons
        if it can be.
        """
        reasons = []
        if isinstance(obj, GroupBy):
            if not _is_batch_upload_needed(obj):
                reasons.append(
                    "GroupBys should not be materialized if batch upload job is not needed"
                )
            # Otherwise group_bys included in online join or are marked explicitly
            # online itself are materialized.
            elif not any(
                join.metaData.online for join in self._get_old_joins_with_group_by(obj)
            ) and not _is_batch_upload_needed(obj):
                reasons.append(
                    "is not marked online/production nor is included in any online join"
                )
        return reasons

    def validate_obj(self, obj: object) -> List[BaseException]:
        """
        Validate Chronon API obj against other entities in the repo.

        returns:
          list of errors.
        """
        if isinstance(obj, GroupBy):
            return self._validate_group_by(obj)
        elif isinstance(obj, Join):
            return self._validate_join(obj)
        return []

    def _has_diff(
        self, obj: object, old_obj: object, skipped_fields=SKIPPED_FIELDS
    ) -> bool:
        new_json = {
            k: v
            for k, v in json.loads(thrift_simple_json(obj)).items()
            if k not in skipped_fields
        }
        old_json = {
            k: v
            for k, v in json.loads(thrift_simple_json(old_obj)).items()
            if k not in skipped_fields
        }
        if isinstance(obj, Join):
            _filter_skipped_fields_from_join(new_json, skipped_fields)
            _filter_skipped_fields_from_join(old_json, skipped_fields)

        return new_json != old_json

    def safe_to_overwrite(self, obj: object) -> bool:
        """When an object is already materialized as online, it is no more safe
        to materialize and overwrite the old conf.
        """
        old_obj = self._get_old_obj(type(obj), obj.metaData.name)
        return (
            not old_obj
            or not self._has_diff(obj, old_obj)
            or not old_obj.metaData.online
        )

    def _validate_derivations(
        self, pre_derived_cols: List[str], derivations: List[Derivation]
    ) -> List[BaseException]:
        """
        Validate join/groupBy's derivation is defined correctly.

        Returns:
          list of validation errors.
        """
        errors = []
        derived_columns = set(pre_derived_cols)

        wild_card_derivation_included = any(
            derivation.expression == "*" for derivation in derivations
        )
        if not wild_card_derivation_included:
            derived_columns.clear()
        for derivation in derivations:
            # if the derivation is a renaming derivation, check whether the expression is in pre-derived schema
            if is_identifier(derivation.expression):
                # for wildcard derivation we want to remove the original column if there is a renaming operation
                # applied on it
                if wild_card_derivation_included:
                    if derivation.expression in derived_columns:
                        derived_columns.remove(derivation.expression)
                if (
                    derivation.expression not in pre_derived_cols
                    and derivation.expression not in ("ds", "ts")
                ):
                    errors.append(
                        ValueError("Incorrect derivation expression {}, expression not found in pre-derived columns {}"
                        .format(
                            derivation.expression, pre_derived_cols
                        ))
                    )
            if derivation.name != "*":
                if derivation.name in derived_columns:
                    errors.append(
                        ValueError("Incorrect derivation name {} due to output column name conflict".format(
                            derivation.name
                        )
                    ))
                else:
                    derived_columns.add(derivation.name)
        return errors

    def _validate_join_part_keys(self, join_part: JoinPart, left_cols: List[str]) -> BaseException:
        keys = []

        key_mapping = join_part.keyMapping if join_part.keyMapping else {}
        for key in join_part.groupBy.keyColumns:
            keys.append(key_mapping.get(key, key))

        missing = [k for k in keys if k not in left_cols]

        err_string = ""
        left_cols_as_str = ", ".join(left_cols)
        group_by_name = join_part.groupBy.metaData.name
        if missing:
            key_mapping_str = f"Key Mapping: {key_mapping}" if key_mapping else ""
            err_string += textwrap.dedent(f"""
                - Join is missing keys {missing} on left side. Required for JoinPart: {group_by_name}. 
                Existing columns on left side: {left_cols_as_str}
                All required Keys: {join_part.groupBy.keyColumns}
                {key_mapping_str}
                Consider renaming a column on the left, or including the key_mapping argument to your join_part.""")

        if key_mapping:
            # Left side of key mapping should include columns on the left
            key_map_keys_missing_from_left = [k for k in key_mapping.keys() if k not in left_cols]
            if key_map_keys_missing_from_left:
                err_string += f"\n- The following keys in your key_mapping: {str(key_map_keys_missing_from_left)} for JoinPart {group_by_name} are not included in the left side of the join: {left_cols_as_str}"

            # Right side of key mapping should only include keys in GroupBy
            keys_missing_from_key_map_values = [v for v in key_mapping.values() if v not in join_part.groupBy.keyColumns]
            if keys_missing_from_key_map_values:
                err_string += f"\n- The following values in your key_mapping: {str(keys_missing_from_key_map_values)}  for JoinPart {group_by_name} do not cover any group by key columns: {join_part.groupBy.keyColumns}"

            if key_map_keys_missing_from_left or keys_missing_from_key_map_values:
                err_string += "\n(Key Mapping should be formatted as column_from_left -> group_by_key)"

        if err_string:
            return ValueError(err_string)


    def _validate_keys(self, join: Join) -> List[BaseException]:
        left = join.left

        left_selects = None
        if left.events:
            left_selects = left.events.query.selects
        elif left.entities:
            left_selects = left.entities.query.selects
        elif left.joinSource:
            left_selects = left.joinSource.query.selects
            # TODO -- if selects are not selected here, get output cols from join

        left_cols = []

        if left_selects:
            left_cols = left_selects.keys()

        errors = []

        if left_cols:
            join_parts = list(join.joinParts)  # Create a copy to avoid modifying the original

            # Add label_parts to join_parts to validate if set
            label_parts = join.labelParts
            if label_parts:
                for label_jp in label_parts.labels:
                    join_parts.append(label_jp)

            # Validate join_parts
            for join_part in join_parts:
                join_part_err = self._validate_join_part_keys(join_part, left_cols)
                if join_part_err:
                    errors.append(join_part_err)

        return errors


    def _validate_join(self, join: Join) -> List[BaseException]:
        """
        Validate join's status with materialized versions of group_bys
        included by the join.

        Returns:
          list of validation errors.
        """
        included_group_bys_and_prefixes = [(rp.groupBy, rp.prefix) for rp in join.joinParts]
        # TODO: Remove label parts check in future PR that deprecates label_parts
        included_label_parts_and_prefixes = [(lp.groupBy, lp.prefix) for lp in join.labelParts.labels] if join.labelParts else []
        included_group_bys = [tup[0] for tup in included_group_bys_and_prefixes]

        offline_included_group_bys = [
            gb.metaData.name
            for gb in included_group_bys
            if not gb.metaData or gb.metaData.online is False
        ]
        errors = []
        old_group_bys = [
            group_by
            for group_by in included_group_bys
            if self._get_old_obj(GroupBy, group_by.metaData.name)
        ]
        non_prod_old_group_bys = [
            group_by.metaData.name
            for group_by in old_group_bys
            if group_by.metaData.production is False
        ]
        # Check if the underlying groupBy is valid
        group_by_errors = [
            self._validate_group_by(group_by) for group_by in included_group_bys
        ]
        errors += [
            ValueError(f"join {join.metaData.name}'s underlying {error}")
            for errors in group_by_errors
            for error in errors
        ]
        # Check if the production join is using non production groupBy
        if join.metaData.production and non_prod_old_group_bys:
            errors.append(
                ValueError("join {} is production but includes the following non production group_bys: {}".format(
                    join.metaData.name, ", ".join(non_prod_old_group_bys)
                )
            ))
        # Check if the online join is using the offline groupBy
        if join.metaData.online:
            if offline_included_group_bys:
                errors.append(
                    ValueError("join {} is online but includes the following offline group_bys: {}".format(
                        join.metaData.name, ", ".join(offline_included_group_bys)
                    )
                ))
        # Only validate the join derivation when the underlying groupBy is valid
        group_by_correct = all(not errors for errors in group_by_errors)
        if join.derivations and group_by_correct:
            features = list(get_pre_derived_join_features(join).keys())
            # For online joins keys are not included in output schema
            if join.metaData.online:
                columns = features
            else:
                keys = list(get_pre_derived_source_keys(join.left).keys())
                columns = features + keys
            errors.extend(self._validate_derivations(columns, join.derivations))


        errors.extend(self._validate_keys(join))

        # If the join is using "short" names, ensure that there are no collisions
        if join.useLongNames is False:
            right_part_collisions = detect_feature_name_collisions(included_group_bys_and_prefixes, "right parts", join.metaData.name)
            if right_part_collisions:
                errors.append(right_part_collisions)

            label_part_collisions = detect_feature_name_collisions(included_label_parts_and_prefixes, "label parts", join.metaData.name)
            if label_part_collisions:
                errors.append(label_part_collisions)

        return errors
    


    def _validate_group_by(self, group_by: GroupBy) -> List[BaseException]:
        """
        Validate group_by's status with materialized versions of joins
        including the group_by.

        Return:
          List of validation errors.
        """
        joins = self._get_old_joins_with_group_by(group_by)
        online_joins = [
            join.metaData.name for join in joins if join.metaData.online is True
        ]
        prod_joins = [
            join.metaData.name for join in joins if join.metaData.production is True
        ]
        errors = []

        non_temporal = (
            group_by.accuracy is None or group_by.accuracy == Accuracy.SNAPSHOT
        )

        no_topic = not _group_by_has_topic(group_by)
        has_hourly_windows = _group_by_has_hourly_windows(group_by)

        # batch features cannot contain hourly windows
        if (no_topic and non_temporal) and has_hourly_windows:
            errors.append(
                ValueError(f"group_by {group_by.metaData.name} is defined to be daily refreshed but contains "
                           f"hourly windows. "
                           )
            )

        def _validate_bounded_event_source():
            if group_by.aggregations is None:
                return
            
            unbounded_event_sources = [
                str(src)
                for src in group_by.sources 
                if isinstance(get_root_source(src), EventSource) and get_query(src).startPartition is None
            ]

            if not unbounded_event_sources:
                return
            
            unwindowed_aggregations = [str(agg) for agg in group_by.aggregations if agg.windows is None]

            if not unwindowed_aggregations:
                return
            
            nln = "\n"

            errors.append(
                ValueError(
                    f"""group_by {group_by.metaData.name} uses unwindowed aggregations [{nln}{f",{nln}".join(unwindowed_aggregations)}{nln}] 
                    on unbounded event sources: [{nln}{f",{nln}".join(unbounded_event_sources)}{nln}]. 
                    Please set a start_partition on the source, or a window on the aggregation.""")
                )
            
        _validate_bounded_event_source()

        # group by that are marked explicitly offline should not be present in
        # materialized online joins.
        if group_by.metaData.online is False and online_joins:
            errors.append(
                ValueError("group_by {} is explicitly marked offline but included in "
                "the following online joins: {}".format(
                    group_by.metaData.name, ", ".join(online_joins)
                ))
            )
        # group by that are marked explicitly non-production should not be
        # present in materialized production joins.
        if prod_joins:
            if group_by.metaData.production is False:
                errors.append(
                    ValueError(
                        "group_by {} is explicitly marked as non-production but included in the following production "
                    "joins: {}".format(group_by.metaData.name, ", ".join(prod_joins))
                ))
            # if the group by is included in any of materialized production join,
            # set it to production in the materialized output.
            else:
                group_by.metaData.production = True

        # validate the derivations are defined correctly
        if group_by.derivations:
            # For online group_by keys are not included in output schema
            if group_by.metaData.online:
                columns = list(get_pre_derived_group_by_features(group_by).keys())
            else:
                columns = list(get_pre_derived_group_by_columns(group_by).keys())
            errors.extend(self._validate_derivations(columns, group_by.derivations))

        for source in group_by.sources:
            src: Source = source
            if (
                src.events
                and src.events.isCumulative
                and (src.events.query.timeColumn is None)
            ):
                errors.append(
                    ValueError("Please set query.timeColumn for Cumulative Events Table: {}".format(
                        src.events.table
                    ))
                )
            elif (
                src.joinSource
            ):
                join_obj = src.joinSource.join
                if join_obj.metaData.name is None or join_obj.metaData.team is None:
                    errors.append(
                        ValueError(f"Join must be defined with team and name: {join_obj}")
                    )
        return errors

    def validate_changes(self, results):
        """
        Validate changes in compiled objects against existing materialized objects.
        
        Args:
            results: List of CompiledObj from parse_configs
            
        Returns:
            None (exits on user cancellation)
        """
        from ai.chronon.cli.compile.display.console import console
        
        # Filter out results with errors and only process GroupBy/Join
        valid_results = []
        for result in results:
            if result.errors:
                continue  # Skip results with errors
            if result.obj_type not in ["GroupBy", "Join", "StagingQuery"]:
                continue  # Skip non-GroupBy/Join objects
            valid_results.append(result)
        
        # Categorize changes with version awareness
        changed_objects = {
            'changed': [],
            'deleted': [],
            'added': [],
            'version_bumped': []
        }
        
        # Process each valid result
        for result in valid_results:
            obj_name = result.obj.metaData.name
            obj_type = result.obj_type
            
            # Check if object exists in old objects
            old_obj = self._get_old_obj(type(result.obj), obj_name)
            
            if old_obj is None:
                # New object
                change = self._create_config_change(result.obj, obj_type)
                changed_objects['added'].append(change)
            elif self._has_diff(result.obj, old_obj):
                # Modified object
                change = self._create_config_change(result.obj, obj_type)
                changed_objects['changed'].append(change)
        
        # Check for deleted objects and detect version changes
        current_objects = {(result.obj_type, result.obj.metaData.name) for result in valid_results}
        current_base_names = {}  # Maps (obj_type, base_name) -> (obj_name, obj)
        
        # Build mapping of current base names
        for result in valid_results:
            obj_name = result.obj.metaData.name
            obj_type = result.obj_type
            base_name, version = self._parse_name_and_version(obj_name)
            current_base_names[(obj_type, base_name)] = (obj_name, result.obj)
        
        for obj_type in ["GroupBy", "Join", "StagingQuery"]:
            old_objs = self.old_objs.get(obj_type, {})
            for obj_name, old_obj in old_objs.items():
                if (obj_type, obj_name) not in current_objects:
                    # Check if this is a version change
                    old_base_name, old_version = self._parse_name_and_version(obj_name)
                    
                    if (obj_type, old_base_name) in current_base_names:
                        # This is a version change
                        new_obj_name, new_obj = current_base_names[(obj_type, old_base_name)]
                        _, new_version = self._parse_name_and_version(new_obj_name)
                        
                        change = self._create_version_change(old_obj, new_obj, obj_type, old_base_name, old_version, new_version)
                        changed_objects['version_bumped'].append(change)
                        
                        # Remove from added list if it was there
                        changed_objects['added'] = [
                            c for c in changed_objects['added'] 
                            if c.name != new_obj_name
                        ]
                    else:
                        # Object was deleted
                        change = self._create_config_change(old_obj, obj_type)
                        changed_objects['deleted'].append(change)
        
        # Report changes
        self._report_changed_objects(changed_objects)
        
        # Check if we need user confirmation (only for non-version-bump changes)
        non_version_changes = self._filter_non_version_changes(
            changed_objects['changed'] + changed_objects['deleted'],
            changed_objects['added']
        )
        
        if non_version_changes:
            if not self._prompt_user_confirmation():
                console.print("‚ùå Compilation cancelled by user.")
                sys.exit(1)
    
    def _parse_name_and_version(self, name: str) -> Tuple[str, int]:
        """Parse config name to extract base name and version.
        
        Args:
            name: Config name (e.g., 'config_name__1' or 'config_name')
            
        Returns:
            Tuple of (base_name, version) where version is None if no version suffix
        """
        if '__' in name:
            parts = name.rsplit('__', 1)
            if len(parts) == 2 and parts[1].isdigit():
                return parts[0], int(parts[1])
        return name, None
    
    def _filter_non_version_changes(self, existing_changes, added_changes):
        """Filter out version changes from existing changes.
        
        Returns list of changes that are NOT version bumps and require confirmation.
        """
        added_names = {change.name for change in added_changes}
        non_version_changes = []
        
        for change in existing_changes:
            # Check if this deleted config has a corresponding added config that represents a version bump
            is_version_bump = any(
                is_version_change(change.name, added_name) 
                for added_name in added_names
            )
            
            if not is_version_bump:
                non_version_changes.append(change)
        
        return non_version_changes
    
    def _create_config_change(self, obj, obj_type):
        """Create a ConfigChange object from a thrift object."""
        base_name, version = self._parse_name_and_version(obj.metaData.name)
        return ConfigChange(
            name=obj.metaData.name,
            obj_type=obj_type,
            online=obj.metaData.online if obj.metaData.online else False,
            production=obj.metaData.production if obj.metaData.production else False,
            base_name=base_name,
            old_version=version,
            new_version=version
        )
    
    def _create_version_change(self, old_obj, new_obj, obj_type, base_name, old_version, new_version):
        """Create a ConfigChange object for a version bump."""
        return ConfigChange(
            name=f"{base_name} (v{old_version} -> v{new_version})",
            obj_type=obj_type,
            online=new_obj.metaData.online if new_obj.metaData.online else False,
            production=new_obj.metaData.production if new_obj.metaData.production else False,
            base_name=base_name,
            old_version=old_version,
            new_version=new_version,
            is_version_change=True
        )
    
    def _report_changed_objects(self, changed_objects):
        """Report categorized changed objects to the user."""
        from ai.chronon.cli.compile.display.console import console
        
        total_existing_changes = len(changed_objects['changed']) + len(changed_objects['deleted'])
        total_new_objects = len(changed_objects['added'])
        total_version_changes = len(changed_objects['version_bumped'])
        
        if total_existing_changes == 0 and total_new_objects == 0 and total_version_changes == 0:
            console.print("\n‚úÖ No changes detected in compiled objects.")
            return
        
        # Helper function to format config name with flags
        def format_config_name(change: ConfigChange) -> str:
            name = f"[{change.obj_type}] {change.name}"
            flags = []
            if change.online:
                flags.append("[bold red][ONLINE][/bold red]")
            if change.production:
                flags.append("[bold magenta][PRODUCTION][/bold magenta]")
            if flags:
                return f"{name} {' '.join(flags)}"
            return name
        
        # Report changed objects
        if changed_objects['changed']:
            console.print(f"\nüîÑ [bold yellow]CHANGED configs ({len(changed_objects['changed'])} objects):[/bold yellow]")
            for change in changed_objects['changed']:
                console.print(f"  ‚Ä¢ {format_config_name(change)}")
        
        # Report deleted objects
        if changed_objects['deleted']:
            console.print(f"\nüóëÔ∏è  [bold red]DELETED configs ({len(changed_objects['deleted'])} objects):[/bold red]")
            for change in changed_objects['deleted']:
                console.print(f"  ‚Ä¢ {format_config_name(change)}")
        
        # Report added objects
        if changed_objects['added']:
            console.print(f"\n‚ûï [bold green]ADDED configs ({len(changed_objects['added'])} objects):[/bold green]")
            for change in changed_objects['added']:
                console.print(f"  ‚Ä¢ {format_config_name(change)}")
        
        # Store version changes to report later
        self._version_changes = changed_objects['version_bumped']
    
    def report_version_changes(self):
        """Report version changes at the end of compilation."""
        from ai.chronon.cli.compile.display.console import console
        
        if hasattr(self, '_version_changes') and self._version_changes:
            # Helper function to format config name with flags
            def format_config_name(change: ConfigChange) -> str:
                name = f"[{change.obj_type}] {change.name}"
                flags = []
                if change.online:
                    flags.append("[bold red][ONLINE][/bold red]")
                if change.production:
                    flags.append("[bold magenta][PRODUCTION][/bold magenta]")
                if flags:
                    return f"{name} {' '.join(flags)}"
                return name
            
            console.print(f"\nüîÑ [bold blue]VERSION BUMPED configs ({len(self._version_changes)} objects):[/bold blue]")
            for change in self._version_changes:
                console.print(f"  ‚Ä¢ {format_config_name(change)}")
    
    def _prompt_user_confirmation(self) -> bool:
        """
        Prompt user for Y/N confirmation to proceed with overwriting existing configs.
        Returns True if user confirms, False otherwise.
        """
        from ai.chronon.cli.compile.display.console import console
        
        console.print("\n‚ùì [bold]Do you want to proceed with overwriting these existing compiled configs?[/bold]")
        console.print("[dim](Version bumps above do not require confirmation and will proceed automatically)[/dim]")
        console.print("[bold]Continue? (y/N):[/bold]", end=" ")
        
        try:
            response = input().strip().lower()
            return response in ['y', 'yes']
        except (EOFError, KeyboardInterrupt):
            console.print("\n‚ùå Compilation cancelled.")
            return False
